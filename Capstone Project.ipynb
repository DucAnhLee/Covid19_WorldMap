{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project using public covid19 datasets from github to create a data warehouse to help with building reports and analyze covid pandemic. A use case of this is to generate a worldmap represents the spread of covid pandemic, and the progress of vaccine from every country. From that we can identify the center of pandemic and make decision to minimize the damage of it.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydeequ\n",
      "  Downloading https://files.pythonhosted.org/packages/64/25/8fb417ac850ddf052297a52e33a90b5000f6b6925fb3a9bcfbd3e1c5ca0a/pydeequ-1.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: pandas>=0.23.0 in /opt/conda/lib/python3.6/site-packages (from pydeequ) (0.23.3)\n",
      "Collecting numpy>=1.14.1 (from pydeequ)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 2.8MB/s eta 0:00:01   22% |███████▎                        | 3.1MB 27.8MB/s eta 0:00:01    88% |████████████████████████████▍   | 11.9MB 26.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.23.0->pydeequ) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.23.0->pydeequ) (2017.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas>=0.23.0->pydeequ) (1.11.0)\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "Installing collected packages: numpy, pydeequ\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed numpy-1.19.5 pydeequ-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SPARK_VERSION=2.4.3\n"
     ]
    }
   ],
   "source": [
    "%env SPARK_VERSION=2.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from  pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from os import walk\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\", pydeequ.deequ_maven_coord).\\\n",
    "config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord).\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "# config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "# config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This project gather public covid19 data from github, I try to combine information from 2 repositories to create a single source of truth, a data warehouse that can later be used in report or analytics data.\n",
    "\n",
    "I will use spark standalone here to process the data, we can easily migrate into running spark on any cloud provider to speed up processing. The following steps will be carried out:\n",
    "\n",
    "- Exploratory data analysis of data on covid19 by Our World in Data to identify data schema and strategies for data cleaning \n",
    "- Exploratory data analysis of covid data at Johns Hopkins University to identify data schema and strategies for data cleaning\n",
    "- Perform data cleaning on all datasets\n",
    "- Identify the grain of fact table is by datetime and by location.\n",
    "- Create datetime and location dimension tables derived from 2 datasets\n",
    "- Create fact table from 2 datasets and reference to dimension tables.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Thanks to [Data on COVID-19 (coronavirus) by Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data) has public covid data daily updated for every country, it include information about vaccine and patients in ICU treament. The data is in csv files with 62 columns, I will not use all columns in this, instead I select some of the columns that I find valuable. I will name it covid_country because its data is at country level. Here is columns description:\n",
    "\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`iso_code`|ISO 3166-1 alpha-3 – three-letter country codes|\n",
    "|`location`|Geographical location|\n",
    "|`date`|Date of observation|\n",
    "|`total_cases`|Total confirmed cases of COVID-19|\n",
    "|`new_cases`|New confirmed cases of COVID-19|\n",
    "|`total_deaths`|Total deaths attributed to COVID-19|\n",
    "|`new_deaths`|New deaths attributed to COVID-19|\n",
    "|`icu_patients`|Number of COVID-19 patients in intensive care units (ICUs) on a given day|\n",
    "|`hosp_patients`|Number of COVID-19 patients in hospital on a given day|\n",
    "|`total_vaccinations`|Total number of COVID-19 vaccination doses administered|\n",
    "|`people_vaccinated`|Total number of people who received at least one vaccine dose|\n",
    "|`people_fully_vaccinated`|Total number of people who received all doses prescribed by the vaccination protocol|\n",
    "|`new_vaccinations`|New COVID-19 vaccination doses administered (only calculated for consecutive days)|\n",
    "\n",
    "Thanks to [COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University](https://github.com/CSSEGISandData/COVID-19) has public covid19 data detail in province and number of recovered patients. Here is detail information about the data. I will call it covid_province because its data is detail at province level.\n",
    "\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`Province_State`|Province, state or dependency name|\n",
    "|`Country_Region`|Country, region or sovereignty name. The names of locations included on the Website correspond with the official designations used by the U.S. Department of State|\n",
    "|`Last_Update`|Has different format in files, we have to format it|\n",
    "|`Confirmed`|Counts include confirmed and probable (where reported)|\n",
    "|`Deaths`|Counts include confirmed and probable (where reported)|\n",
    "|`Recovered`|Recovered cases are estimates based on local media reports, and state and local reporting when available, and therefore may be substantially lower than the true number|\n",
    "\n",
    "Last but not least is location data, it is used to create location table and reference to location of 2 datasets above, it is the combination of [World Cities Database](https://simplemaps.com/data/world-cities) and [JHU CSSE COVID-19 Dataset](https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv)\n",
    "and the location from covid19 datasets but not in 2 location datasets above. Sample location schema from World Cities Database\n",
    "\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`city`|City name|\n",
    "|`city_ascii`|City name in ascii|\n",
    "|`lat`|Latitude of location|\n",
    "|`lon`|Longitude of location|\n",
    "|`country`|Country, region or sovereignty name|\n",
    "|`iso2`|iso2 code of location|\n",
    "|`iso3`|iso3 code of location|\n",
    "|`admin_name`|The name of the highest level administration region of the city town (e.g. a US state or Canadian province).|\n",
    "|`population`|Population of location|\n",
    "|`id`|ID of location|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_code</th>\n",
       "      <th>continent</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>new_cases</th>\n",
       "      <th>new_cases_smoothed</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smoothed</th>\n",
       "      <th>...</th>\n",
       "      <th>extreme_poverty</th>\n",
       "      <th>cardiovasc_death_rate</th>\n",
       "      <th>diabetes_prevalence</th>\n",
       "      <th>female_smokers</th>\n",
       "      <th>male_smokers</th>\n",
       "      <th>handwashing_facilities</th>\n",
       "      <th>hospital_beds_per_thousand</th>\n",
       "      <th>life_expectancy</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>excess_mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>597.029</td>\n",
       "      <td>9.59</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.746</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.511</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  iso_code continent     location        date total_cases new_cases  \\\n",
       "0      AFG      Asia  Afghanistan  2020-02-24         1.0       1.0   \n",
       "1      AFG      Asia  Afghanistan  2020-02-25         1.0       0.0   \n",
       "2      AFG      Asia  Afghanistan  2020-02-26         1.0       0.0   \n",
       "3      AFG      Asia  Afghanistan  2020-02-27         1.0       0.0   \n",
       "4      AFG      Asia  Afghanistan  2020-02-28         1.0       0.0   \n",
       "\n",
       "  new_cases_smoothed total_deaths new_deaths new_deaths_smoothed  \\\n",
       "0               None         None       None                None   \n",
       "1               None         None       None                None   \n",
       "2               None         None       None                None   \n",
       "3               None         None       None                None   \n",
       "4               None         None       None                None   \n",
       "\n",
       "        ...        extreme_poverty cardiovasc_death_rate diabetes_prevalence  \\\n",
       "0       ...                   None               597.029                9.59   \n",
       "1       ...                   None               597.029                9.59   \n",
       "2       ...                   None               597.029                9.59   \n",
       "3       ...                   None               597.029                9.59   \n",
       "4       ...                   None               597.029                9.59   \n",
       "\n",
       "  female_smokers male_smokers handwashing_facilities  \\\n",
       "0           None         None                 37.746   \n",
       "1           None         None                 37.746   \n",
       "2           None         None                 37.746   \n",
       "3           None         None                 37.746   \n",
       "4           None         None                 37.746   \n",
       "\n",
       "  hospital_beds_per_thousand life_expectancy human_development_index  \\\n",
       "0                        0.5           64.83                   0.511   \n",
       "1                        0.5           64.83                   0.511   \n",
       "2                        0.5           64.83                   0.511   \n",
       "3                        0.5           64.83                   0.511   \n",
       "4                        0.5           64.83                   0.511   \n",
       "\n",
       "  excess_mortality  \n",
       "0             None  \n",
       "1             None  \n",
       "2             None  \n",
       "3             None  \n",
       "4             None  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data of Data on COVID-19 (coronavirus) by Our World in Data\n",
    "covid_country_df = spark.read.option(\"header\", \"true\").csv(\"./lake/raw_zone/covid_country/owid-covid-data.csv\")\n",
    "covid_country_df.createOrReplaceTempView(\"raw_covid_country\")\n",
    "covid_country_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+------------+------------------+-------------------+\n",
      "|FIPS|Admin2|Province_State|Country_Region|Last_Update        |Lat     |Long_    |Confirmed|Deaths|Recovered|Active|Combined_Key|Incident_Rate     |Case_Fatality_Ratio|\n",
      "+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+------------+------------------+-------------------+\n",
      "|null|null  |null          |Afghanistan   |2021-02-27 05:22:28|33.93911|67.709953|55696    |2442  |49285    |3969  |Afghanistan |143.0731404659654 |4.384515943694341  |\n",
      "|null|null  |null          |Albania       |2021-02-27 05:22:28|41.1533 |20.1683  |105229   |1756  |68007    |35466 |Albania     |3656.5779414830768|1.6687415066188978 |\n",
      "|null|null  |null          |Algeria       |2021-02-27 05:22:28|28.0339 |1.6596   |112805   |2977  |77842    |31986 |Algeria     |257.2458766830244 |2.639067417224414  |\n",
      "|null|null  |null          |Andorra       |2021-02-27 05:22:28|42.5063 |1.5218   |10822    |110   |10394    |318   |Andorra     |14006.341810651653|1.0164479763444834 |\n",
      "|null|null  |null          |Angola        |2021-02-27 05:22:28|-11.2027|17.8739  |20759    |504   |19307    |948   |Angola      |63.16202375030837 |2.42786261380606   |\n",
      "+----+------+--------------+--------------+-------------------+--------+---------+---------+------+---------+------+------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data of CCSE at Johns Hopkins University\n",
    "covid_province_df = spark.read.option(\"header\", \"true\").csv('./lake/raw_zone/covid_province/')\n",
    "covid_province_df.createOrReplaceTempView(\"raw_covid_province\")\n",
    "spark.sql(\"select * from raw_covid_province\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------+-----------+----+----+-----------+-------+----------+----------+\n",
      "|city   |city_ascii|lat    |lng     |country    |iso2|iso3|admin_name |capital|population|id        |\n",
      "+-------+----------+-------+--------+-----------+----+----+-----------+-------+----------+----------+\n",
      "|Tokyo  |Tokyo     |35.6897|139.6922|Japan      |JP  |JPN |Tōkyō      |primary|37977000  |1392685764|\n",
      "|Jakarta|Jakarta   |-6.2146|106.8451|Indonesia  |ID  |IDN |Jakarta    |primary|34540000  |1360771077|\n",
      "|Delhi  |Delhi     |28.6600|77.2300 |India      |IN  |IND |Delhi      |admin  |29617000  |1356872604|\n",
      "|Mumbai |Mumbai    |18.9667|72.8333 |India      |IN  |IND |Mahārāshtra|admin  |23355000  |1356226629|\n",
      "|Manila |Manila    |14.6000|120.9833|Philippines|PH  |PHL |Manila     |primary|23088000  |1608618140|\n",
      "+-------+----------+-------+--------+-----------+----+----+-----------+-------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample location of province level from world city datasets\n",
    "province_df = spark.read.option(\"header\", \"true\").csv('./lake/raw_zone/country_lookup/worldcities.csv')\n",
    "province_df.createOrReplaceTempView(\"province_df\")\n",
    "spark.sql(\"select * from province_df\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-----+----+------+--------------+--------------+--------+---------+------------+----------+\n",
      "|UID|iso2|iso3|code3|FIPS|Admin2|Province_State|Country_Region|Lat     |Long_    |Combined_Key|Population|\n",
      "+---+----+----+-----+----+------+--------------+--------------+--------+---------+------------+----------+\n",
      "|4  |AF  |AFG |4    |null|null  |null          |Afghanistan   |33.93911|67.709953|Afghanistan |38928341  |\n",
      "|8  |AL  |ALB |8    |null|null  |null          |Albania       |41.1533 |20.1683  |Albania     |2877800   |\n",
      "|12 |DZ  |DZA |12   |null|null  |null          |Algeria       |28.0339 |1.6596   |Algeria     |43851043  |\n",
      "|20 |AD  |AND |20   |null|null  |null          |Andorra       |42.5063 |1.5218   |Andorra     |77265     |\n",
      "|24 |AO  |AGO |24   |null|null  |null          |Angola        |-11.2027|17.8739  |Angola      |32866268  |\n",
      "+---+----+----+-----+----+------+--------------+--------------+--------+---------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample location of province + country level from covid19 datasets\n",
    "country_df = spark.read.option(\"header\", \"true\").csv('./lake/raw_zone/country_lookup/UID_ISO_FIPS_LookUp_Table.csv')\n",
    "country_df.createOrReplaceTempView(\"country_df\")\n",
    "spark.sql(\"select * from country_df\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Data for province and country is duplidate between locations and covid_daily, I will create dim_location table to store location information and reference location uid from dim_location back to covid_province (I will call it fact_covid_province now). For country that has Province_State is null is because that row represent the country average latitude and longitude, and total population of that country, I will replace null with empty province so it will not cause unexpected behaviors when we doing join later.\n",
    "\n",
    "Covid_country dataframe has some columns with None value (etc: `total_deaths`, `new_deaths`,...). I will replace None with 0 indicate that there are no deaths on that day. It will help us easy to do aggregation later.\n",
    "\n",
    "After investigate covid_province data, there are mismatch schema between csv files, for example: \n",
    "\n",
    "In `./lake/raw_zone/covid_province/01-25-2020.csv` we have this schema: ![old schema](./img/01-25-2020.png)\n",
    "\n",
    "In `./lake/raw_zone/covid_province/01-01-2021.csv` we have this schema: ![new schema](./img/01-01-2021.png)\n",
    "\n",
    "I will process this schema mismatch in data, merge into one schema to use later. I also handle the mismatch in datetime format between `Last_Update` and `last Update`  columns.\n",
    "\n",
    "In most data files, `Last Update` is in this format `M/d/yy HH:mm`, but in some files like `./lake/raw_zone/covid_province/03-15-2020.csv` it has format like this `yyyy-MM-ddTHH:mm:ss`\n",
    "\n",
    "Meanwhile `Last_Update` is different between files, some file is `M/d/yy HH:mm` and some is `yyyy-MM-dd HH:mm:ss`.\n",
    "\n",
    "\n",
    "#### Cleaning Steps\n",
    "##### 1. Merge mismatch schema in csv data of covid data detail in provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "covid_province_schema = StructType([\n",
    "    StructField(\"Last_Update\", StringType(), True),\n",
    "    StructField(\"Province_State\", StringType(), True),\n",
    "    StructField(\"Country_Region\", StringType(), True),\n",
    "    StructField(\"Confirmed\", IntegerType(), True),\n",
    "    StructField(\"Deaths\", IntegerType(), True),\n",
    "    StructField(\"Recovered\", IntegerType(), True),\n",
    "    StructField(\"Raw_Last_Update\", StringType(), True)\n",
    "])\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "covid_province_df = spark.createDataFrame(emptyRDD,covid_province_schema)\n",
    "\n",
    "\n",
    "filenames = next(walk(\"./lake/raw_zone/covid_province\"), (None, None, []))[2]\n",
    "for file in filenames:\n",
    "    rawDf = spark.read.option(\"header\", \"true\").csv(f\"./lake/raw_zone/covid_province/{file}\")\n",
    "    if \"Province/State\" in rawDf.schema.names:\n",
    "        \n",
    "        covid_province_df = covid_province_df.union(rawDf.selectExpr(\"case when year(`Last Update`) is null then date_format(from_unixtime(unix_timestamp(`Last Update`, 'M/d/yy HH:mm')), 'yyyy-MM-dd') else date_format(`Last Update`, 'yyyy-MM-dd') end as Last_Update\",\n",
    "                                                                     \"coalesce(`Province/State`,'') as Province_State\", \n",
    "                                                                     \"case when `Country/Region` = 'Mainland China' then 'China' else `Country/Region` end as Country_Region\", \n",
    "                                                                     \"cast(Confirmed as int) as Confirmed\", \"cast(Deaths as int) as Deaths\", \n",
    "                                                                     \"cast(Recovered as int) as Recovered\", \"`Last Update` as Raw_Last_Update\"))\n",
    "    else:\n",
    "        covid_province_df = covid_province_df.union(rawDf.selectExpr(\"case when year(Last_Update) is null then date_format(from_unixtime(unix_timestamp(`Last_Update`, 'M/d/yy HH:mm')), 'yyyy-MM-dd') else date_format(Last_Update, 'yyyy-MM-dd') end as Last_Update\",\n",
    "                                                                     \"coalesce(Province_State, '') as Province_State\", \n",
    "                                                                     \"case when `Country_Region` = 'Mainland China' then 'China' else `Country_Region` end Country_Region\", \n",
    "                                                                     \"cast(Confirmed as int) as Confirmed\", \"cast(Deaths as int) as Deaths\", \"cast(Recovered as int) as Recovered\",\n",
    "                                                                     \"Last_Update as Raw_Last_Update\"))\n",
    "\n",
    "\n",
    "covid_province_df.na.fill(0, subset=[\"confirmed\", \"deaths\", \"recovered\"]).write.mode(\"overwrite\").parquet(\"./lake/work_zone/covid_province\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2. Replace null value in covid_country_df by 0, also change data type of integer columns to integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "covid_country_df = spark.sql(\"\"\"\n",
    "select iso_code iso3, \n",
    "       case when `location` = 'United States' then 'US' else `location` end country_region, \n",
    "       date, \n",
    "       cast(total_cases as int) total_cases, \n",
    "       cast(new_cases as int) new_cases, \n",
    "       cast(total_deaths as int) total_deaths, \n",
    "       cast(new_deaths as int) new_deaths, \n",
    "       cast(icu_patients as int) icu_patients,\n",
    "       cast(hosp_patients as int) hosp_patients, \n",
    "       cast(total_vaccinations as int) total_vaccinations, \n",
    "       cast(people_vaccinated as int) people_vaccinated, \n",
    "       cast(people_fully_vaccinated as int) people_fully_vaccinated, \n",
    "       cast(new_vaccinations as int) new_vaccinations\n",
    "from raw_covid_country\n",
    "where continent is not null\n",
    "\"\"\").na.fill(value=0)\n",
    "covid_country_df.createOrReplaceTempView(\"covid_country\")\n",
    "covid_country_df.write.mode(\"overwrite\").parquet(\"./lake/work_zone/covid_country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The schema will look like this:\n",
    "\n",
    "![Covid warehouse schema](./img/covid_datawarehouse_schema.png)\n",
    "\n",
    "The dim_datetime dimension table is coming from all possible datetime appear in covid_country and covid_province. I prepopulate addition columns that might need in report or analysis in the future, maybe we will want to get day or month in string, or roll up to analyze data in quarter, year.\n",
    "\n",
    "The dim_location table is populated from raw_locations dataset, it includes iso code of the country, the cordination and population. It allow we design a report for different country, in different day, and visualize that useful information to a worldmap. It bring us the overall picture of covid pandemic in all over the world in an intuitive way. We can also calculate the percent of people who get vaccine of a country, from that we can see how that country is dealing with covid pandemic.\n",
    "\n",
    "The fact_covid_country table is derived from covid_country datasets, it brings us daily data of a country, it includes the number of total cases, new cases, total deaths,... of covid19. It also includes the number of patients in ICU (intensive care unit), number of people who get vaccine. From that data, we can visualize in report the spread and damage that covid bring to a country.\n",
    "\n",
    "The fact_covid_province table is coming from covid_province dataset, it drills down the data detail in province level, help us identify the center of pandemic in a country. It also includes the number of recovered patient in province level.\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps to get the final schema are as follow:\n",
    "1. Gather data source from github\n",
    "2. Load data into spark dataframe\n",
    "3. Merge mistmatch data in covid_country dataset \n",
    "4. Cast data type of columns in covid_country into its correct one, replace null value with appropriate number.\n",
    "5. Create dim_datetime table derived from time columns in covid_province and covid_country datasets\n",
    "6. Create dim_location from raw_locations dataset, filter out unnecessary columns\n",
    "7. Select only valuable columns to create fact_covid_country and fact_covid_province tables, reference datetime and location key to 2 appropriate dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[iso3: string, country_region: string, date: string, total_cases: int, new_cases: int, total_deaths: int, new_deaths: int, icu_patients: int, hosp_patients: int, total_vaccinations: int, people_vaccinated: int, people_fully_vaccinated: int, new_vaccinations: int]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read raw_data in work_zone area to start create model\n",
    "covid_province_df = spark.read.parquet(\"./lake/work_zone/covid_province\")\n",
    "covid_province_df.createOrReplaceTempView(\"covid_province\")\n",
    "covid_province_df.cache()\n",
    "\n",
    "covid_country_df = spark.read.parquet(\"./lake/work_zone/covid_country\")\n",
    "covid_country_df.createOrReplaceTempView(\"covid_country\")\n",
    "covid_country_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StructField, StructType\n",
    "\n",
    "# This function is used to create surrogate key for dataframe\n",
    "def dfZipWithIndex (df, offset=1, colName=\"rowId\"):\n",
    "    '''\n",
    "        Ref: https://stackoverflow.com/questions/30304810/dataframe-ified-zipwithindex\n",
    "        Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe \n",
    "        and preserves a schema\n",
    "\n",
    "        :param df: source dataframe\n",
    "        :param offset: adjustment to zipWithIndex()'s index\n",
    "        :param colName: name of the index column\n",
    "    '''\n",
    "\n",
    "    new_schema = StructType(\n",
    "                    [StructField(colName,LongType(),True)]        # new added field in front\n",
    "                    + df.schema.fields                            # previous schema\n",
    "                )\n",
    "\n",
    "    zipped_rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "    new_rdd = zipped_rdd.map(lambda row: ([row[1] +offset] + list(row[0])))\n",
    "\n",
    "    return spark.createDataFrame(new_rdd, new_schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dim_datetime\n",
    "datetime_df = spark.sql(\"\"\"\n",
    "    select distinct date_format(date, \"yyyyMMdd\") as date_sk,\n",
    "           date,\n",
    "           dayofmonth(date) as day,\n",
    "           date_format(date, 'E') as day_short,\n",
    "           date_format(date, 'EEEE') as day_full,\n",
    "           month(date) as month,\n",
    "           date_format(date, 'MMM') as month_short,\n",
    "           date_format(date, 'MMMM') as month_full,\n",
    "           quarter(date) as quarter,\n",
    "           year(date) as year\n",
    "    from covid_country\n",
    "    union\n",
    "    select distinct date_format(last_update, \"yyyyMMdd\") as date_sk,\n",
    "           last_update as date,\n",
    "           dayofmonth(last_update) as day,\n",
    "           date_format(last_update, 'E') as day_short,\n",
    "           date_format(last_update, 'EEEE') as day_full,\n",
    "           month(last_update) as month,\n",
    "           date_format(last_update, 'MMM') as month_short,\n",
    "           date_format(last_update, 'MMMM') as month_full,\n",
    "           quarter(last_update) as quarter,\n",
    "           year(last_update) as year\n",
    "    from covid_province\n",
    "\"\"\")\n",
    "datetime_df.createOrReplaceTempView(\"dim_datetime\")\n",
    "datetime_df.write.mode(\"overwrite\").parquet(\"./lake/gold_zone/dimension/dim_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get location data from covid19 datasets\n",
    "spark.sql(\"\"\"\n",
    "    select distinct case when t2.iso3 is null then '' else t2.iso3 end iso3,\n",
    "           t1.province_state, \n",
    "           t1.country_region\n",
    "    from\n",
    "        (select distinct '' province_state, country_region from covid_country\n",
    "            union\n",
    "        select distinct province_state, country_region from covid_province\n",
    "        ) t1 \n",
    "    left join (select distinct iso3, country_region from covid_country) t2 \n",
    "        on t1.country_region = t2.country_region\n",
    "\"\"\").createOrReplaceTempView(\"tv_location_from_covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dim_location dimension table\n",
    "# Get location data from worldcities dataset\n",
    "spark.sql(\"\"\"\n",
    "    select iso2, \n",
    "           iso3, \n",
    "           coalesce(admin_name, country) as province_state, \n",
    "           case when country = 'United States' then 'US' else country end as country_region, \n",
    "           cast(avg(lat) as decimal(29,4)) as lat, \n",
    "           cast(avg(lng) as decimal(29,4)) as lon, \n",
    "           cast(sum(population) as int) population\n",
    "    from province_df\n",
    "    group by iso2, iso3, province_state, country_region\n",
    "\"\"\").createOrReplaceTempView(\"tv_province\")\n",
    "\n",
    "# Get location from UID_ISO_FIPS_LookUp_Table dataset\n",
    "spark.sql(\"\"\"\n",
    "   select iso2, \n",
    "          iso3, \n",
    "          coalesce(province_state,'') as province_state, \n",
    "          country_region, \n",
    "          cast(lat as decimal(29,4)) as lat, \n",
    "          cast(`long_` as decimal(29,4)) as lon, \n",
    "          cast(population as int) population  \n",
    "    from country_df\n",
    "\"\"\").createOrReplaceTempView(\"tv_country\")\n",
    "\n",
    "# Merge two location datasets into one\n",
    "spark.sql(\"\"\"\n",
    "    select case when t1.iso2 is null then t2.iso2 else t1.iso2 end as iso2, \n",
    "           case when t1.iso3 is null then t2.iso3 else t1.iso3 end as iso3, \n",
    "           case when t1.province_state is null then t2.province_state else t1.province_state end as province_state,\n",
    "           case when t1.country_region is null then t2.country_region else t1.country_region end as country_region,\n",
    "           case when t1.lat is null then t2.lat else t1.lat end lat, \n",
    "           case when t1.lon is null then t2.lon else t1.lon end lon, \n",
    "           case when t1.population is null then t2.population else t1.population end as population\n",
    "    from tv_province t1\n",
    "        full join\n",
    "    tv_country t2 \n",
    "        on t1.iso3 = t2.iso3 and t1.province_state = t2.province_state\n",
    "\"\"\").createOrReplaceTempView(\"tv_location_from_datasets\")\n",
    "\n",
    "# Get updates location data from covid19 datasets\n",
    "spark.sql(\"\"\"\n",
    "    select '' iso2, t1.iso3, \n",
    "            t1.province_state, \n",
    "            t1.country_region, \n",
    "            cast(181 as decimal(29,4)) as lat,\n",
    "            cast(181 as decimal(29,4)) as lon, \n",
    "            -1 as population\n",
    "    from tv_location_from_covid19 t1 \n",
    "        anti join \n",
    "    tv_location_from_datasets t2 \n",
    "        on t1.province_state = t2.province_state and t1.country_region = t2.country_region\n",
    "      --on t1.province_state = t2.province_state and t1.iso3 = t2.iso3 and t1.country_region = t2.country_region\n",
    "\"\"\").createOrReplaceTempView(\"tv_locations_updates\")\n",
    "\n",
    "# Update location dataframe using 2 location datasets and covid19 datasets\n",
    "location_df = spark.sql(\"\"\"\n",
    "    select * from tv_location_from_datasets\n",
    "    union\n",
    "    select * from tv_locations_updates\n",
    "\"\"\").na.fill(value=-1, subset=['population'])\n",
    "\n",
    "# Create dim_location table with surrogate key\n",
    "location_df = dfZipWithIndex(location_df, colName=\"id\")\n",
    "location_df.createOrReplaceTempView(\"dim_location\")\n",
    "location_df.write.mode(\"overwrite\").format(\"parquet\").save(\"./lake/gold_zone/dimension/dim_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact_covid_country table\n",
    "fact_covid_country_df = spark.sql(\"\"\"\n",
    "    select distinct id as location_sk, \n",
    "           cast(date_format(date, 'yyyyMMdd') as int) as date_sk, \n",
    "           total_cases, \n",
    "           new_cases, \n",
    "           total_deaths, \n",
    "           new_deaths, \n",
    "           icu_patients, \n",
    "           hosp_patients, \n",
    "           total_vaccinations, \n",
    "           people_vaccinated, \n",
    "           people_fully_vaccinated, \n",
    "           new_vaccinations\n",
    "    from covid_country cc \n",
    "        left join \n",
    "    dim_location dl\n",
    "        on cc.country_region = dl.country_region\n",
    "\"\"\")\n",
    "fact_covid_country_df.createOrReplaceTempView(\"fact_covid_country\")\n",
    "fact_covid_country_df.write.mode(\"overwrite\").format(\"parquet\").save(\"./lake/gold_zone/fact/fact_covid_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact_covid_province table\n",
    "fact_covid_province_df = spark.sql(\"\"\"\n",
    "    select distinct cast(date_format(last_update, 'yyyyMMdd') as int) as date_sk, \n",
    "           id as location_sk,\n",
    "           confirmed as total_cases, \n",
    "           Deaths as total_deaths, \n",
    "           Recovered as recovered\n",
    "    from covid_province cp\n",
    "        left join \n",
    "    dim_location dl\n",
    "        on cp.province_state = dl.province_state and cp.country_region = dl.country_region\n",
    "\"\"\")\n",
    "fact_covid_province_df.createOrReplaceTempView(\"fact_covid_province\")\n",
    "fact_covid_province_df.write.mode(\"overwrite\").format(\"parquet\").save(\"./lake/gold_zone/fact/fact_covid_province\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "In order to make sure data quality after cleaning and transforming data in warehouse, I perform some data quality check using pydeequ, an open-source tool developed and used at Amazon to verify data quality of many large datasets. \n",
    "\n",
    "There are some actions I run to check the data quality:\n",
    "- Check uniqueness of key in dimension table\n",
    "- Check number of records in dim_location after derived from raw_locations\n",
    "- Check data type of some important table columns \n",
    "\n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks \n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "# Check quality of dim_datetime\n",
    "dim_datetime_df = spark.read.parquet(\"./lake/gold_zone/dimension/dim_datetime\")\n",
    "checkDatetimeSuit = VerificationSuite(spark) \\\n",
    "    .onData(dim_datetime_df) \\\n",
    "    .addCheck(\n",
    "        Check(spark, CheckLevel.Warning, \"Check dim_datetime\").isComplete(\"date_sk\")  \\\n",
    "        .isUnique(\"date_sk\")  \\\n",
    "        .isContainedIn(\"day_short\", [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]) \\\n",
    "        .isContainedIn(\"day_full\", [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]) \\\n",
    "        .hasMin(\"month\", lambda x: x == 1) \\\n",
    "        .hasMax(\"month\", lambda x: x == 12) \\\n",
    "        .isComplete(\"month_short\")  \\\n",
    "        .isContainedIn(\"month_short\", [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]) \\\n",
    "        .isComplete(\"month_full\")  \\\n",
    "        .isContainedIn(\"month_full\", [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]) \\\n",
    "        .hasMin(\"quarter\", lambda x: x == 1) \\\n",
    "        .hasMax(\"quarter\", lambda x: x == 4) \\\n",
    "    )\\\n",
    "    .run()\n",
    "\n",
    "# Check quality of dim_location\n",
    "dim_location_df = spark.read.parquet(\"./lake/gold_zone/dimension/dim_location\")\n",
    "checkLocationSuit = VerificationSuite(spark) \\\n",
    "    .onData(dim_location_df) \\\n",
    "    .addCheck(Check(spark, CheckLevel.Warning, \"Check dim_location\")\\\n",
    "        .isComplete(\"id\")  \\\n",
    "        .isUnique(\"id\")  \\\n",
    "        .hasDataType(\"lat\", ConstrainableDataTypes.Fractional)\\\n",
    "        .hasDataType(\"lon\", ConstrainableDataTypes.Fractional)\\\n",
    "        .hasDataType(\"population\", ConstrainableDataTypes.Integral)\\\n",
    "    )\\\n",
    "    .run()\n",
    "\n",
    "# Check quality of fact_covid_province\n",
    "fact_covid_province_df = spark.read.parquet(\"./lake/gold_zone/fact/fact_covid_province\")\n",
    "checkCovidProvinceSuit = VerificationSuite(spark) \\\n",
    "    .onData(fact_covid_province_df) \\\n",
    "    .addCheck(Check(spark, CheckLevel.Warning, \"Check fact_covid_province\").isComplete(\"date_sk\")  \\\n",
    "        .isComplete(\"location_sk\")  \\\n",
    "        .hasDataType(\"total_cases\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"total_deaths\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"recovered\", ConstrainableDataTypes.Integral)\\\n",
    "    )\\\n",
    "    .run()\n",
    "\n",
    "# Check quality of fact_covid_country\n",
    "fact_covid_country_df = spark.read.parquet(\"./lake/gold_zone/fact/fact_covid_country\")\n",
    "checkCovidCountrySuit = VerificationSuite(spark) \\\n",
    "    .onData(fact_covid_country_df) \\\n",
    "    .addCheck(Check(spark, CheckLevel.Warning, \"Check fact_covid_country\").isComplete(\"date_sk\")  \\\n",
    "        .isComplete(\"location_sk\")  \\\n",
    "        .hasDataType(\"total_cases\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"new_cases\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"total_deaths\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"new_deaths\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"icu_patients\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"hosp_patients\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"total_vaccinations\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"people_vaccinated\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"people_fully_vaccinated\", ConstrainableDataTypes.Integral)\\\n",
    "        .hasDataType(\"new_vaccinations\", ConstrainableDataTypes.Integral)\\\n",
    "    )\\\n",
    "    .run()\n",
    "\n",
    "checkCovidCountryResultDF = VerificationResult.checkResultsAsDataFrame(spark, checkCovidCountrySuit)\n",
    "checkCovidProvinceResultDF = VerificationResult.checkResultsAsDataFrame(spark, checkCovidProvinceSuit)\n",
    "checkDatetimeResultDF = VerificationResult.checkResultsAsDataFrame(spark, checkDatetimeSuit)\n",
    "checkLocationResultDF = VerificationResult.checkResultsAsDataFrame(spark, checkLocationSuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+------------+--------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|check                   |check_level|check_status|constraint                                                                                        |constraint_status|constraint_message|\n",
      "+------------------------+-----------+------------+--------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|Check fact_covid_country|Warning    |Success     |CompletenessConstraint(Completeness(date_sk,None))                                                |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |CompletenessConstraint(Completeness(location_sk,None))                                            |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(total_cases,None),<function1>,Some(<function1>),None)            |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(new_cases,None),<function1>,Some(<function1>),None)              |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(total_deaths,None),<function1>,Some(<function1>),None)           |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(new_deaths,None),<function1>,Some(<function1>),None)             |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(icu_patients,None),<function1>,Some(<function1>),None)           |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(hosp_patients,None),<function1>,Some(<function1>),None)          |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(total_vaccinations,None),<function1>,Some(<function1>),None)     |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(people_vaccinated,None),<function1>,Some(<function1>),None)      |Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(people_fully_vaccinated,None),<function1>,Some(<function1>),None)|Success          |                  |\n",
      "|Check fact_covid_country|Warning    |Success     |AnalysisBasedConstraint(DataType(new_vaccinations,None),<function1>,Some(<function1>),None)       |Success          |                  |\n",
      "+------------------------+-----------+------------+--------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkCovidCountryResultDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----------+------------+---------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|check                    |check_level|check_status|constraint                                                                             |constraint_status|constraint_message|\n",
      "+-------------------------+-----------+------------+---------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|Check fact_covid_province|Warning    |Success     |CompletenessConstraint(Completeness(date_sk,None))                                     |Success          |                  |\n",
      "|Check fact_covid_province|Warning    |Success     |CompletenessConstraint(Completeness(location_sk,None))                                 |Success          |                  |\n",
      "|Check fact_covid_province|Warning    |Success     |AnalysisBasedConstraint(DataType(total_cases,None),<function1>,Some(<function1>),None) |Success          |                  |\n",
      "|Check fact_covid_province|Warning    |Success     |AnalysisBasedConstraint(DataType(total_deaths,None),<function1>,Some(<function1>),None)|Success          |                  |\n",
      "|Check fact_covid_province|Warning    |Success     |AnalysisBasedConstraint(DataType(recovered,None),<function1>,Some(<function1>),None)   |Success          |                  |\n",
      "+-------------------------+-----------+------------+---------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkCovidProvinceResultDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(date_sk,No...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(date_sk),...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(day_short cont...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(day_full conta...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>MinimumConstraint(Minimum(month,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>MaximumConstraint(Maximum(month,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(month_shor...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(month_short co...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>CompletenessConstraint(Completeness(month_full...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>ComplianceConstraint(Compliance(month_full con...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>MinimumConstraint(Minimum(quarter,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Check dim_datetime</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Success</td>\n",
       "      <td>MaximumConstraint(Maximum(quarter,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 check check_level check_status  \\\n",
       "0   Check dim_datetime     Warning      Success   \n",
       "1   Check dim_datetime     Warning      Success   \n",
       "2   Check dim_datetime     Warning      Success   \n",
       "3   Check dim_datetime     Warning      Success   \n",
       "4   Check dim_datetime     Warning      Success   \n",
       "5   Check dim_datetime     Warning      Success   \n",
       "6   Check dim_datetime     Warning      Success   \n",
       "7   Check dim_datetime     Warning      Success   \n",
       "8   Check dim_datetime     Warning      Success   \n",
       "9   Check dim_datetime     Warning      Success   \n",
       "10  Check dim_datetime     Warning      Success   \n",
       "11  Check dim_datetime     Warning      Success   \n",
       "\n",
       "                                           constraint constraint_status  \\\n",
       "0   CompletenessConstraint(Completeness(date_sk,No...           Success   \n",
       "1   UniquenessConstraint(Uniqueness(List(date_sk),...           Success   \n",
       "2   ComplianceConstraint(Compliance(day_short cont...           Success   \n",
       "3   ComplianceConstraint(Compliance(day_full conta...           Success   \n",
       "4              MinimumConstraint(Minimum(month,None))           Success   \n",
       "5              MaximumConstraint(Maximum(month,None))           Success   \n",
       "6   CompletenessConstraint(Completeness(month_shor...           Success   \n",
       "7   ComplianceConstraint(Compliance(month_short co...           Success   \n",
       "8   CompletenessConstraint(Completeness(month_full...           Success   \n",
       "9   ComplianceConstraint(Compliance(month_full con...           Success   \n",
       "10           MinimumConstraint(Minimum(quarter,None))           Success   \n",
       "11           MaximumConstraint(Maximum(quarter,None))           Success   \n",
       "\n",
       "   constraint_message  \n",
       "0                      \n",
       "1                      \n",
       "2                      \n",
       "3                      \n",
       "4                      \n",
       "5                      \n",
       "6                      \n",
       "7                      \n",
       "8                      \n",
       "9                      \n",
       "10                     \n",
       "11                     "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkDatetimeResultDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+------------+-------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|check             |check_level|check_status|constraint                                                                           |constraint_status|constraint_message|\n",
      "+------------------+-----------+------------+-------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "|Check dim_location|Warning    |Success     |CompletenessConstraint(Completeness(id,None))                                        |Success          |                  |\n",
      "|Check dim_location|Warning    |Success     |UniquenessConstraint(Uniqueness(List(id),None))                                      |Success          |                  |\n",
      "|Check dim_location|Warning    |Success     |AnalysisBasedConstraint(DataType(lat,None),<function1>,Some(<function1>),None)       |Success          |                  |\n",
      "|Check dim_location|Warning    |Success     |AnalysisBasedConstraint(DataType(lon,None),<function1>,Some(<function1>),None)       |Success          |                  |\n",
      "|Check dim_location|Warning    |Success     |AnalysisBasedConstraint(DataType(population,None),<function1>,Some(<function1>),None)|Success          |                  |\n",
      "+------------------+-----------+------------+-------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkLocationResultDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model.\n",
    "\n",
    "##### fact_covid_country: Table represent data about total cases, deaths and vaccince of the country by day\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`date_sk`|datetime reference to dim_datetime dimension|\n",
    "|`location_sk`|location reference to dim_location dimension|\n",
    "|`total_cases`|Total confirmed cases of COVID-19|\n",
    "|`new_cases`|New confirmed cases of COVID-19|\n",
    "|`total_deaths`|Total deaths attributed to COVID-19|\n",
    "|`new_deaths`|New deaths attributed to COVID-19|\n",
    "|`icu_patients`|Number of COVID-19 patients in intensive care units (ICUs) on a given day|\n",
    "|`hosp_patients`|Number of COVID-19 patients in hospital on a given day|\n",
    "|`total_vaccinations`|Total number of COVID-19 vaccination doses administered|\n",
    "|`people_vaccinated`|Total number of people who received at least one vaccine dose|\n",
    "|`people_fully_vaccinated`|Total number of people who received all doses prescribed by the vaccination protocol|\n",
    "|`new_vaccinations`|New COVID-19 vaccination doses administered (only calculated for consecutive days)|\n",
    "\n",
    "\n",
    "\n",
    "##### fact_covid_province: Table detail in province data about covid19 with recovered case y day\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`date_sk`|datetime reference to dim_datetime dimension|\n",
    "|`location_sk`|location reference to dim_location dimension|\n",
    "|`total_cases`|Counts include confirmed and probable (where reported)|\n",
    "|`total_deaths`|Counts include confirmed and probable (where reported)|\n",
    "|`recovered`|Recovered cases are estimates based on local media reports, and state and local reporting when available, and therefore may be substantially lower than the true number|\n",
    "\n",
    "\n",
    "##### dim_location: Table represent all information related to location like iso code, latitude, longitude\n",
    "|columns|description|\n",
    "|:--|:--|\n",
    "|`id`|ID of location|\n",
    "|`iso2`|iso2 code of location|\n",
    "|`iso3`|iso3 code of location|\n",
    "|`Province_State`|Province, state or dependency name|\n",
    "|`Country_Region`|Country, region or sovereignty name|\n",
    "|`Lat`|Latitude of location|\n",
    "|`Lon`|Longitude of location|\n",
    "|`population`|Population of location|\n",
    "\n",
    "\n",
    "##### `dim_datetime`: datetime dimension table precalculate some useful information\n",
    "|column|description|\n",
    "|:--|:--|\n",
    "|`date_sk`|datetime key in interger type, with yyyyMMdd format|\n",
    "|`date`|datetime value in string type, with yyyy-MM-dd format|\n",
    "|`day`|day of month|\n",
    "|`day_short`|short string represent day value|\n",
    "|`day_full`|full string represent day value|\n",
    "|`month`|month value in integer|\n",
    "|`month_short`|short string represent month value|\n",
    "|`month_full`|full string represent month value|\n",
    "|`quarter`|number of quater in a year|\n",
    "|`year`|year value|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### 1. Technologies used in this project\n",
    "\n",
    "I use mainly is pyspark to clean data, it has ability to handle big data, it can working with many different type of files and it is easy to use. For now it just running standalone. In order to utilize all its power, we can run this project with Spark cluster running distributed on YARN or using databricks platform.\n",
    "\n",
    "In order to perform checking data quality, I use [pydeequ](https://github.com/awslabs/python-deequ), an open source python wrapper of Deequ (an open-source tool developed and used at Amazon). It is very powerful tool when you want to check data quality on big data.\n",
    "\n",
    "##### 2. How often the data should be updated\n",
    "\n",
    "For covid19 datasets, I think it is the best that dataset is updated daily, because it is a resonable time frame to measure all the numbers and events.\n",
    "\n",
    "##### 3. How you would approach the problem differently under the following scenarios:\n",
    "* The data was increased by 100x: Spark can easily handle that but with distributed mode and reasonable number of nodes.\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day: We can use Apache Airflow to schedule the pipeline.\n",
    "* The database needed to be accessed by 100+ people: We can use Amazon Redshift, it has ability to work with large number of people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
